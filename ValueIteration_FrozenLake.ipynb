{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Frozen Lake Domain Description\n","\n","Frozen Lake is a simple grid-world environment where an agent navigates a frozen lake to reach a goal while avoiding falling into holes. The environment is represented as a grid, with each cell being one of the following:\n","\n","* **S**: Starting position of the agent\n","* **F**: Frozen surface, safe to walk on\n","* **H**: Hole, falling into one ends the episode with a reward of 0\n","* **G**: Goal, reaching it ends the episode with a reward of 1\n","\n","The agent can take four actions:\n","\n","* **0: Left**\n","* **1: Down**\n","* **2: Right**\n","* **3: Up**\n","\n","However, due to the slippery nature of the ice, the agent might not always move in the intended direction. There's a chance it moves perpendicular to the intended direction.\n","\n","\n"],"metadata":{"id":"hzTUHNC0Oien"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"nKf_jjk9OgN1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727150946174,"user_tz":420,"elapsed":971,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}},"outputId":"e635a400-8f9d-493e-ebcb-b9e8569ea151"},"outputs":[{"output_type":"stream","name":"stdout","text":["  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]}],"source":["import gym\n","\n","# Create the environment\n","env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n","\n","# Reset the environment to the initial state\n","observation = env.reset()\n","\n","# Take a few actions and observe the results\n","for _ in range(5):\n","    action = env.action_space.sample()  # Choose a random action\n","    observation, reward, done, info = env.step(action)\n","    # Render the environment to see the agent's movement (text-based)\n","    if done:\n","        observation= env.reset()\n","    else:\n","      rendered = env.render()\n","      if len(rendered) > 1:  # Check if there's a second element\n","         print(rendered[1])  # Print the second element\n","# Close the environment\n","env.close()"]},{"cell_type":"markdown","source":["The transition model for the Frozen Lake world describes how the agent's actions affect its movement and the resulting state transitions. Here's a breakdown of the key components:\n","\n","**Actions:**\n","\n","* The agent can choose from four actions:\n","    * 0: Left\n","    * 1: Down\n","    * 2: Right\n","    * 3: Up\n","\n","**State Transitions:**\n","\n","* **Intended Movement:** Ideally, the agent moves one cell in the chosen direction.\n","* **Slippery Ice:** Due to the slippery nature of the ice, there's a probability that the agent will move in a perpendicular direction instead of the intended one. The exact probabilities depend on the specific Frozen Lake environment configuration, but typically:\n","    * **Successful Move:** The agent moves in the intended direction with a high probability.\n","    * **Perpendicular Move:** The agent moves 90 degrees to the left or right of the intended direction with a lower probability.\n","* **Boundaries:** If the intended movement would take the agent outside the grid boundaries, it remains in its current position.\n","* **Holes:** If the agent lands on a hole (\"H\"), the episode ends, and it receives a reward of 0.\n","* **Goal:** If the agent reaches the goal (\"G\"), the episode ends, and it receives a reward of 1.\n","\n","\n"],"metadata":{"id":"R_q5-OvYOldL"}},{"cell_type":"code","source":["import gym\n","\n","# Create the environment\n","env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n","\n","# Reset the environment to the initial state\n","observation = env.reset()\n","\n","# Take a few actions and observe the results\n","for _ in range(5):\n","    action = env.action_space.sample()  # Choose a random action\n","    observation, reward, done, info = env.step(action)\n","    # Render the environment to see the agent's movement (text-based)\n","    if done:\n","        observation= env.reset()\n","    else:\n","      rendered = env.render()\n","      if len(rendered) > 1:  # Check if there's a second element\n","         print(rendered[1])  # Print the second element\n","# Close the environment\n","env.close()\n","print (\"State 14 Going Right: (s, a, r, Done)\", env.P[14][2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7nU_-9uaOQR","outputId":"172c8cc4-b79a-4f34-d3a0-c86893cf6d1e","executionInfo":{"status":"ok","timestamp":1727150950369,"user_tz":420,"elapsed":232,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n","State 14 Going Right: (s, a, r, Done) [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n"]}]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","\n","# Create FrozenLake environment\n","env = gym.make(\"FrozenLake-v1\")\n","\n","# Starter code for students (modified for number of iterations)\n","def value_iteration(env, gamma=0.9, num_iterations=1000):\n","    \"\"\"\n","    Implements the Value Iteration algorithm.\n","\n","    Args:\n","        env: The OpenAI Gym environment.\n","        gamma: Discount factor.\n","        num_iterations: Number of iterations to run.\n","\n","    Returns:\n","        The optimal value function and policy.\n","    \"\"\"\n","\n","    # Initialize value function and policy\n","    V = np.zeros(env.observation_space.n)\n","    policy = np.zeros(env.observation_space.n)\n","\n","    # TODO: Implement the core Value Iteration logic here\n","    # - Iterate for 'num_iterations'\n","    # - For each state:\n","    #   - Calculate Q values for all actions\n","    #   - Update V[s] with the max Q value\n","    #   - Update policy[s] with the action that maximizes Q value\n","\n","    for _ in range(num_iterations):\n","        for s in range(env.observation_space.n):\n","            Q_values = []\n","            for a in range(env.action_space.n):\n","                Q_value = 0\n","                for prob, next_state, reward, done in env.P[s][a]:\n","                    Q_value += prob * (reward + gamma * V[next_state])\n","                Q_values.append(Q_value)\n","            V[s] = max(Q_values)\n","            policy[s] = np.argmax(Q_values)\n","\n","    return V, policy\n","\n","# Apply student's Value Iteration\n","optimal_V, optimal_policy = value_iteration(env)\n","\n","# Print the optimal value function and policy\n","print(\"Optimal Value Function:\")\n","print(np.reshape(optimal_V, (4,4)))\n","print(\"\\nOptimal Policy:\")\n","print(np.reshape(optimal_policy, (4,4)))\n","print(\"\\nAction Mapping:\")\n","print(\"0: Left, 1: Down, 2: Right, 3: Up\")\n","\n","# Evaluate student's solution (Optional)\n","def evaluate_policy(env, policy, num_episodes=100):\n","    total_reward = 0\n","    for _ in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        while not done:\n","            action = policy[state]\n","            state, reward, done, _ = env.step(action)\n","            total_reward += reward\n","    return total_reward / num_episodes\n","\n","average_reward = evaluate_policy(env, optimal_policy)\n","print(\"\\nAverage Reward:\", average_reward)"],"metadata":{"id":"U92a-f0HO1j7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727150954503,"user_tz":420,"elapsed":1333,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}},"outputId":"2c8ec623-c81c-4508-c71d-f0eceee23ba7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Value Function:\n","[[0.0688909  0.06141457 0.07440976 0.05580732]\n"," [0.09185454 0.         0.11220821 0.        ]\n"," [0.14543635 0.24749695 0.29961759 0.        ]\n"," [0.         0.3799359  0.63902015 0.        ]]\n","\n","Optimal Policy:\n","[[0. 3. 0. 3.]\n"," [0. 0. 0. 0.]\n"," [3. 1. 0. 0.]\n"," [0. 2. 1. 0.]]\n","\n","Action Mapping:\n","0: Left, 1: Down, 2: Right, 3: Up\n","\n","Average Reward: 0.71\n"]}]},{"cell_type":"code","source":["# @title Sep 11 In class code discussion\n","\n","'''\n","# value iteration\n","\n","for _ in range(num_iterations):\n","    delta = 0\n","    V_prev = V.copy()\n","    for s in range(env.observation_space.n):\n","        v = V[s]\n","        action_values = []\n","        for a in range(env.action_space.n):\n","            q_value = 0\n","            for prob, next_state, reward, done in env.P[s][a]:\n","                q_value += prob * (reward + gamma * V_prev[next_state])\n","            action_values.append(q_value)\n","\n","        V[s] = max(action_values)\n","        policy[s] = np.argmax(action_values)\n","        delta = max(delta, abs(v - V[s])\n","    if delta < theta and 0:\n","        break\n","    V_prev = V.copy()\n","'''\n","'''\n","# policy iteration\n","\n","# Going through the policy for each state (Take one action only instead of doing all the actions)\n","'''"],"metadata":{"id":"e_eGX7hCrdAQ","cellView":"code"},"execution_count":null,"outputs":[]}]}