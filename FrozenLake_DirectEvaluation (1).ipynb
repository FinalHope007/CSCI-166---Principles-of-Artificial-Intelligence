{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/everestso/Fall24Spring25/blob/main/FrozenLake_DirectEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# Assignment code are written at the bottom of this google collab."],"metadata":{"id":"AJ7rU6NNmGyi"}},{"cell_type":"markdown","source":["# Frozen Lake w/ Value Iteration & Direct Evaluation"],"metadata":{"id":"rTwoHidK7QXR"}},{"cell_type":"markdown","source":["## Frozen Lake Domain Description\n","\n","Frozen Lake is a simple grid-world environment where an agent navigates a frozen lake to reach a goal while avoiding falling into holes. The environment is represented as a grid, with each cell being one of the following:\n","\n","* **S**: Starting position of the agent\n","* **F**: Frozen surface, safe to walk on\n","* **H**: Hole, falling into one ends the episode with a reward of 0\n","* **G**: Goal, reaching it ends the episode with a reward of 1\n","\n","The agent can take four actions:\n","\n","* **0: Left**\n","* **1: Down**\n","* **2: Right**\n","* **3: Up**\n","\n","However, due to the slippery nature of the ice, the agent might not always move in the intended direction. There's a chance it moves perpendicular to the intended direction.\n","\n","\n"],"metadata":{"id":"hzTUHNC0Oien"}},{"cell_type":"code","execution_count":65,"metadata":{"id":"nKf_jjk9OgN1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"027c40c4-5b54-4e75-df06-878b376109d8","executionInfo":{"status":"ok","timestamp":1727157417023,"user_tz":420,"elapsed":261,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  (Right)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n"]}],"source":["import gym\n","\n","# Create the environment\n","env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n","\n","# Reset the environment to the initial state\n","observation = env.reset()\n","\n","# Take a few actions and observe the results\n","for _ in range(5):\n","    action = env.action_space.sample()  # Choose a random action\n","    observation, reward, done, info = env.step(action)\n","    # Render the environment to see the agent's movement (text-based)\n","    if done:\n","        observation= env.reset()\n","    else:\n","      rendered = env.render()\n","      if len(rendered) > 1:  # Check if there's a second element\n","         print(rendered[1])  # Print the second element\n","# Close the environment\n","env.close()"]},{"cell_type":"markdown","source":["The transition model for the Frozen Lake world describes how the agent's actions affect its movement and the resulting state transitions. Here's a breakdown of the key components:\n","\n","**Actions:**\n","\n","* The agent can choose from four actions:\n","    * 0: Left\n","    * 1: Down\n","    * 2: Right\n","    * 3: Up\n","\n","**State Transitions:**\n","\n","* **Intended Movement:** Ideally, the agent moves one cell in the chosen direction.\n","* **Slippery Ice:** Due to the slippery nature of the ice, there's a probability that the agent will move in a perpendicular direction instead of the intended one. The exact probabilities depend on the specific Frozen Lake environment configuration, but typically:\n","    * **Successful Move:** The agent moves in the intended direction with a high probability.\n","    * **Perpendicular Move:** The agent moves 90 degrees to the left or right of the intended direction with a lower probability.\n","* **Boundaries:** If the intended movement would take the agent outside the grid boundaries, it remains in its current position.\n","* **Holes:** If the agent lands on a hole (\"H\"), the episode ends, and it receives a reward of 0.\n","* **Goal:** If the agent reaches the goal (\"G\"), the episode ends, and it receives a reward of 1.\n","\n","\n"],"metadata":{"id":"R_q5-OvYOldL"}},{"cell_type":"code","source":["import gym\n","\n","# Create the environment\n","env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n","\n","# Reset the environment to the initial state\n","observation = env.reset()\n","\n","# Take a few actions and observe the results\n","for _ in range(5):\n","    action = env.action_space.sample()  # Choose a random action\n","    observation, reward, done, info = env.step(action)\n","    # Render the environment to see the agent's movement (text-based)\n","    if done:\n","        observation= env.reset()\n","    else:\n","      rendered = env.render()\n","      if len(rendered) > 1:  # Check if there's a second element\n","         print(rendered[1])  # Print the second element\n","# Close the environment\n","env.close()\n","print (\"State 14 Going Right: (s, a, r, Done)\", env.P[14][2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7nU_-9uaOQR","outputId":"0fdbc1f1-7a80-4cfa-dd5c-e05c18156632","executionInfo":{"status":"ok","timestamp":1727157417193,"user_tz":420,"elapsed":3,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}}},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","\n","State 14 Going Right: (s, a, r, Done) [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n"]}]},{"cell_type":"markdown","source":["# Value Iteration"],"metadata":{"id":"lompMKJo2O2R"}},{"cell_type":"code","source":["import gym\n","import numpy as np\n","\n","def value_iteration(env, gamma=0.9, num_iterations=1000, theta=0.0001):\n","    \"\"\"\n","    Implements the Value Iteration algorithm.\n","\n","    Args:\n","        env: The OpenAI Gym environment.\n","        gamma: Discount factor.\n","        num_iterations: Number of iterations to run.\n","\n","    Returns:\n","        The optimal value function and policy.\n","    \"\"\"\n","    # Initialize value function and policy\n","    V = np.zeros(env.observation_space.n)\n","    policy = np.zeros(env.observation_space.n, dtype=int)\n","\n","    for _ in range(num_iterations):\n","        delta = 0\n","        V_prev = V.copy()\n","        for s in range(env.observation_space.n):\n","            v = V[s]\n","            action_values = []  # Store Q values for all actions in this state\n","\n","            for a in range(env.action_space.n):\n","                q_value = 0\n","                for prob, next_state, reward, done in env.P[s][a]:\n","                    q_value += prob * (reward + gamma * V_prev[next_state])\n","                action_values.append(q_value)\n","\n","            # Update V[s] with the max Q value\n","            V[s] = max(action_values)\n","            # Update policy[s] with the action that maximizes Q value\n","            policy[s] = np.argmax(action_values)\n","\n","            delta = max(delta, abs(v - V[s]))\n","        if delta < theta and 0:\n","            break\n","        V_prev = V.copy()\n","\n","    return V, policy\n","\n","# Create the environment\n","env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n","\n","# Reset the environment to the initial state\n","observation = env.reset()\n","\n","# Apply student's Value Iteration\n","optimal_V, optimal_policy = value_iteration(env)\n","print (f\"optimal policy= \\n{optimal_policy.reshape((4,4))}\\n optimal_V=\\n{np.round(optimal_V.reshape((4,4)), 2)}\")\n","\n","# Evaluate student's solution (Optional)\n","def evaluate_policy(env, policy, num_episodes=100):\n","    total_reward = 0\n","    for _ in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        while not done:\n","            action = policy[state]\n","            state, reward, done, _ =  env.step(action)\n","            total_reward += reward\n","    return total_reward / num_episodes\n","\n","average_reward = evaluate_policy(env, optimal_policy)\n","print(\"Average Reward:\", average_reward)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_kv_pwvY4YR","outputId":"d6fa7966-62d5-4576-dbf8-a0c8bf926dbb","executionInfo":{"status":"ok","timestamp":1727157417838,"user_tz":420,"elapsed":647,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}}},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["optimal policy= \n","[[0 3 0 3]\n"," [0 0 0 0]\n"," [3 1 0 0]\n"," [0 2 1 0]]\n"," optimal_V=\n","[[0.07 0.06 0.07 0.06]\n"," [0.09 0.   0.11 0.  ]\n"," [0.15 0.25 0.3  0.  ]\n"," [0.   0.38 0.64 0.  ]]\n","Average Reward: 0.79\n"]}]},{"cell_type":"markdown","source":["# Policy Extraction & Policy Evaluation"],"metadata":{"id":"9Xc8WhLW2Spz"}},{"cell_type":"code","source":["def policy_extraction(env, V, gamma=0.9):\n","  policy = np.zeros(env.observation_space.n, dtype=int)\n","  for s in range(env.observation_space.n):\n","      v = V[s]\n","      action_values = []  # Store Q values for all actions in this state\n","\n","      for a in range(env.action_space.n):\n","          q_value = 0\n","          for prob, next_state, reward, done in env.P[s][a]:\n","              q_value += prob * (reward + gamma * V[next_state])\n","          action_values.append(q_value)\n","\n","      # Update policy[s] with the action that maximizes Q value\n","      policy[s] = np.argmax(action_values)\n","  return policy\n","\n","def policy_evaluation(env, policy, gamma=0.9, num_iterations=1000, theta=0.0001):\n","    \"\"\"\n","    Implements the Value Iteration algorithm.\n","\n","    Args:\n","        env: The OpenAI Gym environment.\n","        gamma: Discount factor.\n","        num_iterations: Number of iterations to run.\n","\n","    Returns:\n","        The optimal value function and policy.\n","    \"\"\"\n","    # Initialize value function and policy\n","    V = np.zeros(env.observation_space.n)\n","    for _ in range(num_iterations):\n","        delta = 0\n","        V_prev = V.copy()\n","        for s in range(env.observation_space.n):\n","            v = V[s]\n","            a = policy[s]\n","            q_value = 0\n","            for prob, next_state, reward, done in env.P[s][a]:\n","                q_value += prob * (reward + gamma * V_prev[next_state])\n","            # Update V[s] with the max Q value\n","            V[s] = q_value\n","\n","            delta = max(delta, abs(v - V[s]))\n","        if delta < theta and 0:\n","            break\n","        V_prev = V.copy()\n","    return V"],"metadata":{"id":"XFUHBMc1kiOr","executionInfo":{"status":"ok","timestamp":1727157417838,"user_tz":420,"elapsed":4,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["V_policy_evalution = policy_evaluation(env, optimal_policy)\n","print (f\"V_policy_evalution=\\n{V_policy_evalution}\")\n","V_policy = policy_extraction(env, V_policy_evalution)\n","print (f\"optimal policy= \\n{optimal_policy.reshape((4,4))}\\n V_policy=\\n{V_policy.reshape((4,4))}\")"],"metadata":{"id":"_ZxtAYnEmeIS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727157417839,"user_tz":420,"elapsed":4,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}},"outputId":"758fd5e4-8570-4d98-c37b-087218307ed6"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["V_policy_evalution=\n","[0.0688909  0.06141457 0.07440976 0.05580732 0.09185454 0.\n"," 0.11220821 0.         0.14543635 0.24749695 0.29961759 0.\n"," 0.         0.3799359  0.63902015 0.        ]\n","optimal policy= \n","[[0 3 0 3]\n"," [0 0 0 0]\n"," [3 1 0 0]\n"," [0 2 1 0]]\n"," V_policy=\n","[[0 3 0 3]\n"," [0 0 0 0]\n"," [3 1 0 0]\n"," [0 2 1 0]]\n"]}]},{"cell_type":"markdown","source":["# Reinforcement Learning"],"metadata":{"id":"4-nohV0on3fG"}},{"cell_type":"code","source":["env.reset(seed=42)\n","init_policy = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n","print (f\"optimal policy= \\n{init_policy.reshape((4,4))}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9uKUHB9o2dr","outputId":"e554df04-58f6-47bb-f385-7d40440755be","executionInfo":{"status":"ok","timestamp":1727157418119,"user_tz":420,"elapsed":283,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}}},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["optimal policy= \n","[[0 3 0 3]\n"," [0 0 0 0]\n"," [3 1 0 0]\n"," [0 2 1 0]]\n"]}]},{"cell_type":"code","source":["def GenerateEpisodes(env, policy, num_episodes=5):\n","    total_reward = 0\n","    episodes = []\n","    for _ in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        episode = []\n","        while not done:\n","            action = policy[state]\n","            newstate, reward, done, _ = env.step(action)\n","            episode.append((int(state), action, newstate, reward))\n","            state = newstate\n","        episodes.append(episode)\n","    return episodes\n","\n","training_episodes = GenerateEpisodes(env, init_policy)\n","for i, e in enumerate(training_episodes):\n","  print (f\"training_episode=({i=}):\\n{e}\")\n","\n","training_episodes2 = GenerateEpisodes(env, init_policy, 1000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"htwNCLIJpo16","outputId":"d41f8b19-2ca9-46e8-ce22-6102a5e757bf","executionInfo":{"status":"ok","timestamp":1727157419325,"user_tz":420,"elapsed":1217,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}}},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["training_episode=(i=0):\n","[(0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 15, 1.0)]\n","training_episode=(i=1):\n","[(0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 6, 0.0), (6, 0, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 14, 0.0), (14, 1, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 14, 0.0), (14, 1, 15, 1.0)]\n","training_episode=(i=2):\n","[(0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 15, 1.0)]\n","training_episode=(i=3):\n","[(0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 13, 0.0), (13, 2, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 10, 0.0), (10, 0, 6, 0.0), (6, 0, 2, 0.0), (2, 0, 1, 0.0), (1, 3, 1, 0.0), (1, 3, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0)]\n","training_episode=(i=4):\n","[(0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 0, 0.0), (0, 0, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 8, 0.0), (8, 3, 4, 0.0), (4, 0, 8, 0.0), (8, 3, 9, 0.0), (9, 1, 13, 0.0), (13, 2, 14, 0.0), (14, 1, 15, 1.0)]\n"]}]},{"cell_type":"markdown","source":["# Direct Evaluation\n","\n","## Evaluate Single Episode"],"metadata":{"id":"f_yt-sIhrGwJ"}},{"cell_type":"code","source":["def EvaluateEpisode(env, e, V_DE, V_Counts, gamma=0.9):\n","    future_reward = 0\n","    for t in reversed(e):  # Iterate in reverse order\n","        future_reward = t[3] + gamma * future_reward\n","        V_DE[t[0]] = future_reward+V_DE[t[0]]\n","        V_Counts[t[0]] = V_Counts[t[0]]+1\n","    return V_DE, V_Counts"],"metadata":{"id":"jB40XhParIyD","executionInfo":{"status":"ok","timestamp":1727157419326,"user_tz":420,"elapsed":9,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}}},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate Episode 1\n","\n"],"metadata":{"id":"NRQJmwWJ3E0u"}},{"cell_type":"code","source":["V_DE = np.zeros((env.observation_space.n))\n","V_Counts = np.zeros((env.observation_space.n))\n","V_DE, V_Count = EvaluateEpisode(env, training_episodes[0], V_DE, V_Counts, 0.9)\n","V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n","print (f\"V_DE=\\n{V_DE.reshape((4,4))}\")\n","print (f\"V_Counts=\\n{V_Counts.reshape((4,4))}\")\n","print (f\"V=\\n{V.reshape((4,4))}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FaUV7MWJ29oe","outputId":"8ac19f66-1058-43bd-9398-dd785a1b17ac","executionInfo":{"status":"ok","timestamp":1727157419326,"user_tz":420,"elapsed":8,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}}},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["V_DE=\n","[[0.59049 0.      0.      0.     ]\n"," [0.6561  0.      0.      0.     ]\n"," [0.729   0.81    0.9     0.     ]\n"," [0.      0.      1.      0.     ]]\n","V_Counts=\n","[[1. 0. 0. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 1. 1. 0.]\n"," [0. 0. 1. 0.]]\n","V=\n","[[0.59049 0.      0.      0.     ]\n"," [0.6561  0.      0.      0.     ]\n"," [0.729   0.81    0.9     0.     ]\n"," [0.      0.      1.      0.     ]]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-73-000a7187090b>:4: RuntimeWarning: invalid value encountered in divide\n","  V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n"]}]},{"cell_type":"markdown","source":["## Evaluate All Episodes"],"metadata":{"id":"wlbXCln-4EXX"}},{"cell_type":"code","source":["V_DE = np.zeros((env.observation_space.n))\n","V_Counts = np.zeros((env.observation_space.n))\n","for e in training_episodes2:\n","    V_DE, V_Count = EvaluateEpisode(env, e, V_DE, V_Counts, 0.9)\n","V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n","print (f\"V_DE=\\n{V_DE.reshape((4,4))}\")\n","print (f\"V_Counts=\\n{V_Counts.reshape((4,4))}\")\n","print (f\"V_DirectEvaluation=\\n{np.round(V.reshape((4,4)),2)}\")\n","print (f\"optimal policy= \\n{optimal_policy.reshape((4,4))}\\n optimal_V=\\n{np.round(optimal_V.reshape((4,4)), 2)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9OswEPertn95","outputId":"56fda206-4d60-4ca6-90e0-8a1f7b243500","executionInfo":{"status":"ok","timestamp":1727157419326,"user_tz":420,"elapsed":6,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}}},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["V_DE=\n","[[ 879.44773993   12.02033939   32.45370905    0.        ]\n"," [ 910.67996731    0.           63.889207      0.        ]\n"," [ 943.61931212  887.58499323  418.52594351    0.        ]\n"," [   0.         1065.13267976 1369.46029196    0.        ]]\n","V_Counts=\n","[[12819.   227.   503.     0.]\n"," [ 9779.     0.   646.     0.]\n"," [ 6632.  3600.  1432.     0.]\n"," [    0.  2876.  2159.     0.]]\n","V_DirectEvaluation=\n","[[0.07 0.05 0.06 0.  ]\n"," [0.09 0.   0.1  0.  ]\n"," [0.14 0.25 0.29 0.  ]\n"," [0.   0.37 0.63 0.  ]]\n","optimal policy= \n","[[0 3 0 3]\n"," [0 0 0 0]\n"," [3 1 0 0]\n"," [0 2 1 0]]\n"," optimal_V=\n","[[0.07 0.06 0.07 0.06]\n"," [0.09 0.   0.11 0.  ]\n"," [0.15 0.25 0.3  0.  ]\n"," [0.   0.38 0.64 0.  ]]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-74-440a84a75783>:5: RuntimeWarning: invalid value encountered in divide\n","  V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n"]}]},{"cell_type":"markdown","source":["# Assignment starts here"],"metadata":{"id":"sheV06hVhGKo"}},{"cell_type":"code","source":["# @title Q-Learning algorithm with epsilon-greedy action selection\n","\n","def q_learning(env, gamma=0.9, alpha=0.5, epsilon=0.3, num_episodes=5000):\n","    \"\"\"\n","    Implements the Q-Learning algorithm with epsilon-greedy action selection.\n","\n","    Args:\n","        env: The OpenAI Gym environment.\n","        gamma: Discount factor.\n","        alpha: Learning rate.\n","        epsilon: Exploration rate (Probability to take random action).\n","        num_episodes: Number of episodes to run.\n","\n","    Returns:\n","        The optimal Q-function and policy.\n","    \"\"\"\n","\n","    Q = np.zeros((env.observation_space.n, env.action_space.n))\n","    policy = np.zeros(env.observation_space.n, dtype=int)\n","\n","    for _ in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","\n","        while not done:\n","            if np.random.rand() < epsilon:\n","                action = env.action_space.sample()  # Explore\n","            else:\n","                action = np.argmax(Q[state])  # Exploit\n","\n","            next_state, reward, done, _ = env.step(action)\n","            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","            state = next_state\n","\n","    policy = np.argmax(Q, axis=1)\n","    V = np.max(Q, axis=1)\n","\n","    return Q, policy, V\n","\n","# Run Q-learning\n","Q, policy_qlearning, V_qlearning = q_learning(env)\n","\n","# Print results\n","print(f\"Q-learning Policy:\\n{policy_qlearning.reshape((4, 4))}\")\n","print(f\"Q-learning Value Function:\\n{np.round(V_qlearning.reshape((4, 4)), 2)}\\n\")\n","\n","# Compare with Value Iteration results\n","print(f\"Value Iteration Policy:\\n{optimal_policy.reshape((4, 4))}\")\n","print(f\"Value Iteration Value Function:\\n{np.round(optimal_V.reshape((4, 4)), 2)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2WQLu7aSZ6m","executionInfo":{"status":"ok","timestamp":1727157423762,"user_tz":420,"elapsed":4440,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}},"outputId":"42342767-a65e-403e-9b85-43fc347f906f"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-learning Policy:\n","[[0 3 3 3]\n"," [0 0 2 0]\n"," [3 1 1 0]\n"," [0 1 3 0]]\n","Q-learning Value Function:\n","[[0.1  0.08 0.05 0.04]\n"," [0.17 0.   0.09 0.  ]\n"," [0.19 0.43 0.21 0.  ]\n"," [0.   0.71 0.69 0.  ]]\n","\n","Value Iteration Policy:\n","[[0 3 0 3]\n"," [0 0 0 0]\n"," [3 1 0 0]\n"," [0 2 1 0]]\n","Value Iteration Value Function:\n","[[0.07 0.06 0.07 0.06]\n"," [0.09 0.   0.11 0.  ]\n"," [0.15 0.25 0.3  0.  ]\n"," [0.   0.38 0.64 0.  ]]\n"]}]},{"cell_type":"code","source":["# @title Q-Learning algorithm with (1)exploration function and (2)decaying epsilon\n","\n","#(1)exploration function\n","def q_learning_exploration(env, gamma=0.9, alpha=0.5, epsilon=0.3, num_episodes=5000, exploration_func=None, k=1.0):\n","    \"\"\"\n","    Implements the Q-Learning algorithm with visit counts and exploration function.\n","\n","    Args:\n","        env: The OpenAI Gym environment.\n","        gamma: Discount factor.\n","        alpha: Learning rate.\n","        epsilon: Exploration rate.\n","        num_episodes: Number of episodes to run.\n","        exploration_func: A function that takes the value estimate and visit count and returns an exploration value.\n","        k: Exploration constant.\n","\n","    Returns:\n","        The optimal Q-function and policy.\n","    \"\"\"\n","\n","    Q = np.zeros((env.observation_space.n, env.action_space.n))\n","    N = np.zeros((env.observation_space.n, env.action_space.n))  # Visit counts\n","    policy = np.zeros(env.observation_space.n, dtype=int)\n","\n","    for _ in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","\n","        while not done:\n","            if exploration_func is not None:\n","                exploration_values = [exploration_func(Q[state, a], N[state, a], k) for a in range(env.action_space.n)]\n","                action = np.argmax(exploration_values)\n","            else:\n","                if np.random.rand() < epsilon:\n","                    action = env.action_space.sample()  # Explore\n","                else:\n","                    action = np.argmax(Q[state])  # Exploit\n","\n","            next_state, reward, done, _ = env.step(action)\n","            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","            N[state, action] += 1  # Increment visit count\n","            state = next_state\n","\n","    policy = np.argmax(Q, axis=1)\n","    V = np.max(Q, axis=1)\n","\n","    return Q, policy, V\n","\n","def exploration_function(u, n, k=1.0):\n","  \"\"\"\n","  Calculates the exploration function value.\n","\n","  Args:\n","    u: Value estimate.\n","    n: Visit count.\n","    k: Exploration constant.\n","\n","  Returns:\n","    The exploration function value.\n","  \"\"\"\n","  if n == 0:\n","    return float('inf')  # Handle the case of no visits\n","  else:\n","    return u + k / n\n","\n","\n","#(2)decaying epsilon\n","def q_learning_decaying_epsilon(env, gamma=0.9, alpha=0.5, epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.9985, num_episodes=5000):\n","    \"\"\"\n","    Implements the Q-Learning algorithm with decaying epsilon-greedy action selection.\n","\n","    Args:\n","        env: The OpenAI Gym environment.\n","        gamma: Discount factor.\n","        alpha: Learning rate.\n","        epsilon_start: Initial exploration rate.\n","        epsilon_min: Minimum exploration rate.\n","        epsilon_decay: Decay rate for epsilon.\n","        num_episodes: Number of episodes to run.\n","\n","    Returns:\n","        The optimal Q-function and policy.\n","    \"\"\"\n","\n","    Q = np.zeros((env.observation_space.n, env.action_space.n))\n","    policy = np.zeros(env.observation_space.n, dtype=int)\n","    epsilon = epsilon_start\n","\n","    for _ in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","\n","        while not done:\n","            if np.random.rand() < epsilon:\n","                action = env.action_space.sample()  # Explore\n","            else:\n","                action = np.argmax(Q[state])  # Exploit\n","\n","            next_state, reward, done, _ = env.step(action)\n","            Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n","            state = next_state\n","\n","        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n","\n","    policy = np.argmax(Q, axis=1)\n","    V = np.max(Q, axis=1)\n","\n","    return Q, policy, V\n","\n","# Run Q-learning with different configurations\n","#Q, policy_qlearning, V_qlearning = q_learning(env)\n","Q_exploration, policy_exploration, V_exploration = q_learning_exploration(env, exploration_func=exploration_function)\n","Q_decay, policy_decay, V_decay = q_learning_decaying_epsilon(env)\n","\n","# Print results and compare\n","print(\"Original Q-learning:\")\n","print(f\"Policy:\\n{policy_qlearning.reshape((4, 4))}\")\n","print(f\"Value Function:\\n{np.round(V_qlearning.reshape((4, 4)), 2)}\")\n","\n","print(\"\\nQ-learning with Exploration Function:\")\n","print(f\"Policy:\\n{policy_exploration.reshape((4, 4))}\")\n","print(f\"Value Function:\\n{np.round(V_exploration.reshape((4, 4)), 2)}\")\n","\n","print(\"\\nQ-learning with Decaying Epsilon:\")\n","print(f\"Policy:\\n{policy_decay.reshape((4, 4))}\")\n","print(f\"Value Function:\\n{np.round(V_decay.reshape((4, 4)), 2)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-9Wh-ZQZkDj","executionInfo":{"status":"ok","timestamp":1727157434637,"user_tz":420,"elapsed":10882,"user":{"displayName":"Chester Lee","userId":"08616585164992109780"}},"outputId":"f6d1d919-297c-451a-883d-7e20feb4bf2d"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Q-learning:\n","Policy:\n","[[0 3 3 3]\n"," [0 0 2 0]\n"," [3 1 1 0]\n"," [0 1 3 0]]\n","Value Function:\n","[[0.1  0.08 0.05 0.04]\n"," [0.17 0.   0.09 0.  ]\n"," [0.19 0.43 0.21 0.  ]\n"," [0.   0.71 0.69 0.  ]]\n","\n","Q-learning with Exploration Function:\n","Policy:\n","[[1 3 0 3]\n"," [0 0 2 0]\n"," [3 1 0 0]\n"," [0 2 2 0]]\n","Value Function:\n","[[0.08 0.09 0.15 0.  ]\n"," [0.15 0.   0.36 0.  ]\n"," [0.31 0.5  0.73 0.  ]\n"," [0.   0.68 0.9  0.  ]]\n","\n","Q-learning with Decaying Epsilon:\n","Policy:\n","[[1 3 1 3]\n"," [0 0 2 0]\n"," [3 1 0 0]\n"," [0 2 3 0]]\n","Value Function:\n","[[0.06 0.06 0.06 0.05]\n"," [0.07 0.   0.04 0.  ]\n"," [0.22 0.48 0.21 0.  ]\n"," [0.   0.76 0.6  0.  ]]\n"]}]}]}